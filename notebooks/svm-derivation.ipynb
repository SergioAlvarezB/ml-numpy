{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a SVM with gradient descent\n",
    "\n",
    "In this notebook I show how to derivate an algorithm to train a SVM using projected gradient descent. In practice SVM are trained using more nuance algorithms like [SMO](https://en.wikipedia.org/wiki/Sequential_minimal_optimization). However as suggested in [CIML](http://ciml.info/), SVMs can also be trained using projected gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from models.svm import SVM\n",
    "from utils.datasets import blobs_classification_dataset\n",
    "from utils.visualization import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Turn interactive plotting off\n",
    "plt.ioff()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing the margin\n",
    "\n",
    "The most simple version of the SVM is the problem of finding the optimal separating hyperplane between 2 classes which are lineary separable. The optimal plane is that that maximizes the margin M defined as the distance from the plane to its closest point in the training set. Let $f(x) = x^T\\beta + \\beta_0 = 0$ be the separating hyperplane. Then we can solve the optimization problem:\n",
    "$$\n",
    "\\max_{\\beta, \\beta_0, \\|\\beta\\|=1} M \\\\\n",
    "\\text{subject to: } y_i(x_i^T\\beta + \\beta_0) \\geq M, i=1,2,..,N\n",
    "$$\n",
    "where $y_i \\in {-1, 1}$ is the label of the $i$-th training example $x_i$. \n",
    "\n",
    "Noting that the distance from a point $x_i^\\prime$ to a hyperplane $x^T\\beta^\\prime + \\beta_0^\\prime = 0$ is $\\frac{|x_i^{\\prime T}\\beta^\\prime + \\beta_0^\\prime|}{\\|\\beta^\\prime\\|}$, the original problem without noise can be interpreted as maximizing the minimum distance($M$) from each point in the data to the separating plane. Setting $M$ as $\\frac{1}{\\|\\beta\\|}$ the problem can be rewritte as:\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\frac{1}{2}\\|\\beta\\|^2 \\\\\n",
    "\\text{subject to: } y_i(x_i^T\\beta + \\beta_0) \\geq 1, i=1,2,..,N\n",
    "$$\n",
    "\n",
    "The problem is reduced to optimizing a quadratic function, with just one single minimum, subject to some constraint. This constrained optimization problem can be solved using a method called the [Lagrangian multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier), a good explanation is given in [Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization). This method incorporates the constrain along with the objective function into the same expression. Moreover, the Lagrangian multiplier method allows to solve optimization problems with equality constraints, in order to incorporate the inequality constraint the [KKT conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) must also be met. The Lagrange primal function of the problem is:\n",
    "$$\n",
    "L_P = \\frac{1}{2}\\|\\beta\\|^2 - \\sum_{i=1}^N\\alpha_i[y_i(x_i^T\\beta + \\beta_0) - 1]\n",
    "$$\n",
    "\n",
    "Setting derivatives to $0$:\n",
    "$$\n",
    "\\beta = \\sum_{i=1}^{N}\\alpha_iy_ix_i \\\\\n",
    "0 = \\sum_{i=1}^{N}\\alpha_iy_i\n",
    "$$\n",
    "\n",
    "and substituting:\n",
    "$$\n",
    "L_D = \\sum_{i=1}^{N}\\alpha_i - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{i=k}^{N}\\alpha_i\\alpha_ky_iy_kx_i^Tx_k \\\\\n",
    "\\text{subject to: } \\alpha_i \\geq 0, i=1,2,..,N\n",
    "$$\n",
    "\n",
    "The solution is obtained by maximizing $L_D$, which is the dual form. To satisfy the KKT conditions the following constraint:\n",
    "$$\n",
    "\\alpha_i[y_i(x_i^T\\beta + \\beta_0) - 1] = 0 \\hspace{0.2cm}\\forall i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the dual form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with noise: Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
